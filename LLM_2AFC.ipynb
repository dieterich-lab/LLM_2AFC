{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d016ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5843f5ee70439da788dc0c5f2f231a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"openai/gpt-oss-20b\" \n",
    "# MODEL_ID = \"Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d357f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "pattern = re.compile(r\"\\[\\*\\*\\s*(.*?)\\s*\\*\\*\\]\")\n",
    "\n",
    "for filename in os.listdir(\"data/entities_txt\"):\n",
    "    with open(os.path.join(\"data/entities_txt\", filename), 'r') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "        pattern_brat = re.compile(r'^[^\\t]+\\t[^\\t]+\\s+\\d+\\s+\\d+\\t(.+)$')\n",
    "\n",
    "        original_entities = []\n",
    "        fictive_entities = []\n",
    "        with open(os.path.join(\"data/original_ann\", filename), 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                for line in lines:\n",
    "                    match = pattern_brat.match(line.strip())\n",
    "                    if match:\n",
    "                        original_entities.append(match.group(1))\n",
    "                        \n",
    "        with open(os.path.join(\"data/fictive_ann\", filename), 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                for line in lines:\n",
    "                    match = pattern_brat.match(line.strip())\n",
    "                    if match:\n",
    "                        fictive_entities.append(match.group(1))\n",
    "\n",
    "        for m in pattern.finditer(content):\n",
    "            start, end = m.span()\n",
    "            \n",
    "            LETTER = content[:start] + \"[** ENTITY_SLOT **]\" + content[end:]\n",
    "            \n",
    "            # Pull the next pair\n",
    "            original_entity = original_entities.pop(0)\n",
    "            fictive_entity   = fictive_entities.pop(0)\n",
    "\n",
    "            # Randomly decide which option slot gets the real original\n",
    "            if random.random() < 0.5:\n",
    "                OPTION_A, OPTION_B = original_entity, fictive_entity\n",
    "                truth_label, truth_text = \"A\", original_entity\n",
    "            else:\n",
    "                OPTION_A, OPTION_B = fictive_entity, original_entity\n",
    "                truth_label, truth_text = \"B\", original_entity\n",
    "            \n",
    "            SYSTEM_PROMPT = \"\"\"2AFC LINKAGE EVALUATOR\n",
    "            You evaluate a two-alternative forced-choice (2AFC) linkage task on doctor letters.\n",
    "\n",
    "            Input:\n",
    "            • One letter with a single masked slot: [**ENTITY_SLOT**]\n",
    "            • Two candidates labeled A) and B)\n",
    "\n",
    "            Task: Choose the candidate that most likely belongs into masked slot given the context of the letter.\n",
    "\n",
    "            Output (STRICT):\n",
    "            You have to return exactly one capital letter — A or B — with nothing else (no spaces, punctuation, or newlines).\n",
    "            You can not Answer with None.\n",
    "            \"\"\"\n",
    "\n",
    "            # ---- Beispieltext zur Verarbeitung ----\n",
    "            INPUT_TEXT = f\"\"\"\"\n",
    "            DOCTOR LETTER:\n",
    "                {LETTER}\n",
    "            OPTIONS:\n",
    "                A) {OPTION_A}\n",
    "                B) {OPTION_B}\n",
    "            \"\"\"\n",
    "                \n",
    "            def build_input(tokenizer, INPUT_TEXT: str):\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": INPUT_TEXT},\n",
    "                ]\n",
    "                \n",
    "                if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "                    return tokenizer.apply_chat_template(\n",
    "                        messages, add_generation_prompt=True, return_tensors=\"pt\", reasoning_effort=\"low\"\n",
    "                    )\n",
    "                \n",
    "            # ---- Hauptverarbeitung ----\n",
    "            input_ids = build_input(tokenizer, INPUT_TEXT).to(model.device)\n",
    "\n",
    "\n",
    "            gen_kwargs = dict(\n",
    "                max_new_tokens=1000,\n",
    "                do_sample=False,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(input_ids, **gen_kwargs)\n",
    "\n",
    "            # Slice off the prompt part\n",
    "            gen_ids = out[0][input_ids.shape[-1]:]\n",
    "            raw = tokenizer.decode(gen_ids, skip_special_tokens=False).strip()\n",
    "\n",
    "            # Generic: capture ANY channel blocks\n",
    "            def grab_block(text: str, kind: str):\n",
    "                start_tag = f\"<|channel|>{kind}<|message|>\"\n",
    "                i = text.rfind(start_tag)\n",
    "                if i == -1:\n",
    "                    return None\n",
    "                i += len(start_tag)\n",
    "                # find nearest terminator after i\n",
    "                j_end = text.find(\"<|end|>\", i)\n",
    "                j_ret = text.find(\"<|return|>\", i)\n",
    "                j_candidates = [x for x in (j_end, j_ret) if x != -1]\n",
    "                j = min(j_candidates) if j_candidates else len(text)\n",
    "                return text[i:j].strip()\n",
    "\n",
    "            analysis_text = grab_block(raw, \"analysis\")\n",
    "            final_text = grab_block(raw, \"final\")\n",
    "\n",
    "            output_data = {\n",
    "                \"letter\": LETTER,\n",
    "                \"option_a\": OPTION_A,\n",
    "                \"option_b\": OPTION_B,\n",
    "                \"resoning\": analysis_text,\n",
    "                \"choice\": final_text,\n",
    "                \"truth\":truth_label,\n",
    "                \"is_correct\": (final_text == truth_label)\n",
    "            }\n",
    "\n",
    "            output_file = os.path.join(\"LLM_2AFC_output\", f\"result_{os.path.splitext(filename)[0]}_{m.start()}.json\")\n",
    "\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(output_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e1df7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 746\n",
      "Incorrect predictions: 690\n",
      "Accuracy: 51.95%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "directory = \"LLM_2AFC_output\"\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(directory, filename), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if data[\"is_correct\"]:\n",
    "                correct_count += 1\n",
    "            else:\n",
    "                incorrect_count += 1\n",
    "\n",
    "print(f\"Correct predictions: {correct_count}\")\n",
    "print(f\"Incorrect predictions: {incorrect_count}\")\n",
    "print(f\"Accuracy: {correct_count / (correct_count + incorrect_count):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df1cbf",
   "metadata": {},
   "source": [
    "# 2AFC stats from n (trials) and k (correct)\n",
    "- Accuracy\n",
    "- Standard error\n",
    "- Wilson 95% CI\n",
    "- 90% Wilson CI (for equivalence checks)\n",
    "- Binomial test vs 50%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed630d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1436, k = 746\n",
      "Accuracy (p̂)      : 51.95%\n",
      "Std. error         : 0.013184  (1.318 percentage points)\n",
      "95% CI (Wilson (approx.)): [49.36%, 54.53%]\n",
      "Binomial test vs 50%: two-sided p = 0.147 | one-sided p(>0.5) = 0.073\n",
      "90% CI (Wilson 90% (approx.)): [49.78%, 54.11%]\n",
      "Equivalence margin  : ±5.0 pp → window [45.00%, 55.00%]\n",
      "Equivalence result  : PASS (inside window)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "def accuracy(n, k):\n",
    "    return k / n\n",
    "\n",
    "def se_proportion(n, k):\n",
    "    p = k / n\n",
    "    return math.sqrt(p * (1 - p) / n)\n",
    "\n",
    "def wilson_ci(n, k, alpha=0.05):\n",
    "    # Wilson score interval\n",
    "    z = {0.10: 1.2815515655446004, 0.05: 1.6448536269514722, 0.025: 1.959963984540054}[alpha/2]\n",
    "    p = k / n\n",
    "    denom = 1 + (z*z)/n\n",
    "    center = (p + (z*z)/(2*n)) / denom\n",
    "    half = (z * math.sqrt((p*(1-p)/n) + (z*z)/(4*n*n))) / denom\n",
    "    return (center - half, center + half)\n",
    "\n",
    "def normal_binom_test_vs_half(n, k):\n",
    "    \"\"\"\n",
    "    Continuity-corrected normal approx for H0: p = 0.5.\n",
    "    Returns (one_sided_p_gt, two_sided_p).\n",
    "    \"\"\"\n",
    "    mu = n / 2\n",
    "    sigma = math.sqrt(n / 4)\n",
    "    # For upper-tail (k >= observed), use k - 0.5 continuity correction\n",
    "    z = ((k - 0.5) - mu) / sigma\n",
    "    # Survival function of standard normal using erf\n",
    "    def norm_sf(z):\n",
    "        cdf = 0.5 * (1 + math.erf(z / math.sqrt(2)))\n",
    "        return 1 - cdf\n",
    "    one_sided = norm_sf(z)              # P(K >= k | H0)\n",
    "    # two-sided: double the smaller tail\n",
    "    cdf_lower = 1 - one_sided           # P(K <= k-1 | H0, with CC)\n",
    "    two_sided = 2 * min(cdf_lower, one_sided)\n",
    "    return one_sided, two_sided\n",
    "\n",
    "def try_exact_binom(n, k):\n",
    "    \"\"\"\n",
    "    Try to compute exact binomial test & exact (Clopper–Pearson) CI via SciPy.\n",
    "    \"\"\"\n",
    "    from scipy.stats import binomtest, beta\n",
    "    # Exact test vs 0.5\n",
    "    exact_test_two = binomtest(k, n, p=0.5, alternative='two-sided').pvalue\n",
    "    exact_test_one  = binomtest(k, n, p=0.5, alternative='greater').pvalue\n",
    "    # Exact 95% CI (Clopper–Pearson)\n",
    "    alpha = 0.05\n",
    "    if k == 0:\n",
    "        cp_lo = 0.0\n",
    "    else:\n",
    "        cp_lo = beta.ppf(alpha/2, k, n - k + 1)\n",
    "    if k == n:\n",
    "        cp_hi = 1.0\n",
    "    else:\n",
    "        cp_hi = beta.ppf(1 - alpha/2, k + 1, n - k)\n",
    "    # Exact 90% CI\n",
    "    alpha90 = 0.10\n",
    "    if k == 0:\n",
    "        cp_lo_90 = 0.0\n",
    "    else:\n",
    "        cp_lo_90 = beta.ppf(alpha90/2, k, n - k + 1)\n",
    "    if k == n:\n",
    "        cp_hi_90 = 1.0\n",
    "    else:\n",
    "        cp_hi_90 = beta.ppf(1 - alpha90/2, k + 1, n - k)\n",
    "    return {\n",
    "        \"p_two_sided\": exact_test_two,\n",
    "        \"p_one_sided\": exact_test_one,\n",
    "        \"cp95\": (cp_lo, cp_hi),\n",
    "        \"cp90\": (cp_lo_90, cp_hi_90),\n",
    "    }\n",
    "\n",
    "def summarize(n, k, equivalence_margin=0.05):\n",
    "    p_hat = accuracy(n, k)\n",
    "    se = se_proportion(n, k)\n",
    "\n",
    "    # Wilson 95% and 90%\n",
    "    wilson95 = wilson_ci(n, k, alpha=0.05)\n",
    "    wilson90 = wilson_ci(n, k, alpha=0.10)\n",
    "\n",
    "    # Tests vs 50%\n",
    "    exact = try_exact_binom(n, k)\n",
    "    if exact:\n",
    "        p_two = exact[\"p_two_sided\"]\n",
    "        p_one = exact[\"p_one_sided\"]\n",
    "        ci95 = exact[\"cp95\"]\n",
    "        ci90 = exact[\"cp90\"]\n",
    "        ci_label = \"Exact Clopper–Pearson\"\n",
    "        ci90_label = \"Exact 90% (Clopper–Pearson)\"\n",
    "    else:\n",
    "        p_one, p_two = normal_binom_test_vs_half(n, k)\n",
    "        ci95 = wilson95\n",
    "        ci90 = wilson90\n",
    "        ci_label = \"Wilson (approx.)\"\n",
    "        ci90_label = \"Wilson 90% (approx.)\"\n",
    "\n",
    "    # Equivalence check: is the entire 90% CI inside [0.5 ± margin]?\n",
    "    eq_low, eq_high = 0.5 - equivalence_margin, 0.5 + equivalence_margin\n",
    "    equivalent_to_chance = (ci90[0] >= eq_low) and (ci90[1] <= eq_high)\n",
    "\n",
    "    # Pretty print\n",
    "    def pct(x): return f\"{100*x:.2f}%\"\n",
    "    print(f\"n = {n}, k = {k}\")\n",
    "    print(f\"Accuracy (p̂)      : {pct(p_hat)}\")\n",
    "    print(f\"Std. error         : {se:.6f}  ({100*se:.3f} percentage points)\")\n",
    "    print(f\"95% CI ({ci_label}): [{pct(ci95[0])}, {pct(ci95[1])}]\")\n",
    "    print(f\"Binomial test vs 50%: two-sided p = {p_two:.3f} | one-sided p(>0.5) = {p_one:.3f}\")\n",
    "    print(f\"90% CI ({ci90_label}): [{pct(ci90[0])}, {pct(ci90[1])}]\")\n",
    "    print(f\"Equivalence margin  : ±{100*equivalence_margin:.1f} pp → \"\n",
    "          f\"window [{pct(eq_low)}, {pct(eq_high)}]\")\n",
    "    print(f\"Equivalence result  : \"\n",
    "          f\"{'PASS (inside window)' if equivalent_to_chance else 'FAIL (touches/exceeds window)'}\")\n",
    "\n",
    "summarize(n=1436, k=746, equivalence_margin=0.05)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
